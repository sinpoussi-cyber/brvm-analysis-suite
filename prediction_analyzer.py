# ==============================================================================
# MODULE: PREDICTION ANALYZER - ALGORITHME HYBRIDE
# Pr√©diction sur 20 jours ouvr√©s (Mardi-Samedi)
# ==============================================================================

import psycopg2
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import os
import logging
import json
import gspread
from google.oauth2 import service_account
from datetime import datetime, timedelta

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')

# --- Configuration & Secrets ---
DB_NAME = os.environ.get('DB_NAME')
DB_USER = os.environ.get('DB_USER')
DB_PASSWORD = os.environ.get('DB_PASSWORD')
DB_HOST = os.environ.get('DB_HOST')
DB_PORT = os.environ.get('DB_PORT')
GSPREAD_SERVICE_ACCOUNT_JSON = os.environ.get('GSPREAD_SERVICE_ACCOUNT')
SPREADSHEET_ID = os.environ.get('SPREADSHEET_ID')

# --- Connexion PostgreSQL ---
def connect_to_db():
    try:
        conn = psycopg2.connect(
            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD,
            host=DB_HOST, port=DB_PORT
        )
        logging.info("‚úÖ Connexion PostgreSQL r√©ussie.")
        return conn
    except Exception as e:
        logging.error(f"‚ùå Erreur connexion DB: {e}")
        return None

# --- Authentification Google Sheets ---
def authenticate_gsheets():
    try:
        if not GSPREAD_SERVICE_ACCOUNT_JSON:
            return None
        
        creds_dict = json.loads(GSPREAD_SERVICE_ACCOUNT_JSON)
        scopes = ['https://www.googleapis.com/auth/spreadsheets']
        creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=scopes)
        gc = gspread.authorize(creds)
        logging.info("‚úÖ Authentification Google Sheets r√©ussie.")
        return gc
    except Exception as e:
        logging.error(f"‚ùå Erreur authentification Google Sheets: {e}")
        return None

# --- G√©n√©ration des jours ouvr√©s (Mardi-Samedi) ---
def generate_business_days(start_date, num_days=20):
    """
    G√©n√®re les 20 prochains jours ouvr√©s (Mardi-Samedi)
    La BRVM est ouverte du Mardi au Samedi
    """
    business_days = []
    current_date = start_date + timedelta(days=1)
    
    while len(business_days) < num_days:
        # 0=Lundi, 1=Mardi, 2=Mercredi, 3=Jeudi, 4=Vendredi, 5=Samedi, 6=Dimanche
        weekday = current_date.weekday()
        
        # Mardi(1) au Samedi(5)
        if 1 <= weekday <= 5:
            business_days.append(current_date)
        
        current_date += timedelta(days=1)
    
    return business_days

# --- Algorithme Hybride de Pr√©diction ---
def hybrid_prediction(prices, dates):
    """
    Algorithme hybride combinant 3 m√©thodes :
    1. R√©gression Lin√©aire (40%)
    2. Tendance R√©cente (30%)
    3. Moyenne Mobile Pond√©r√©e (30%)
    """
    if len(prices) < 100:
        logging.warning("Pas assez de donn√©es pour pr√©diction (< 100 jours)")
        return None
    
    # Prendre les 100 derniers jours
    prices_100 = prices[-100:].values
    dates_100 = dates[-100:]
    
    # Convertir les dates en nombres (jours depuis le d√©but)
    days = np.arange(len(prices_100)).reshape(-1, 1)
    
    # Derni√®re valeur connue
    last_price = prices_100[-1]
    
    # G√©n√©rer les 20 prochains jours ouvr√©s
    last_date = dates_100.iloc[-1].date()
    future_business_days = generate_business_days(last_date, num_days=20)
    future_days = np.arange(len(prices_100), len(prices_100) + 20).reshape(-1, 1)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTHODE 1 : R√âGRESSION LIN√âAIRE (40%)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    model = LinearRegression()
    model.fit(days, prices_100)
    prediction_linear = model.predict(future_days).flatten()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTHODE 2 : TENDANCE R√âCENTE (30%)
    # Bas√©e sur les 30 derniers jours
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    prices_30 = prices_100[-30:]
    trend_recent = (prices_30[-1] - prices_30[0]) / 30
    prediction_trend = np.array([last_price + trend_recent * (i + 1) for i in range(20)])
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTHODE 3 : MOYENNE MOBILE POND√âR√âE (30%)
    # Poids exponentiels (plus r√©cent = plus important)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    weights = np.exp(np.linspace(-1, 0, 30))
    weights = weights / weights.sum()  # Normaliser
    weighted_avg = np.average(prices_30, weights=weights)
    
    # Extrapoler avec la moyenne pond√©r√©e
    drift = weighted_avg - prices_30[0]
    prediction_weighted = np.array([weighted_avg + drift * (i + 1) / 30 for i in range(20)])
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PR√âDICTION FINALE (Combinaison pond√©r√©e)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    prediction_final = (
        0.4 * prediction_linear +
        0.3 * prediction_trend +
        0.3 * prediction_weighted
    )
    
    # Calculer l'intervalle de confiance (¬± 5%)
    confidence_interval = prediction_final * 0.05
    lower_bound = prediction_final - confidence_interval
    upper_bound = prediction_final + confidence_interval
    
    # Calculer la variation moyenne pr√©vue
    avg_change = (prediction_final[-1] - last_price) / last_price * 100
    
    return {
        'dates': future_business_days,
        'predictions': prediction_final,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'last_price': last_price,
        'avg_change_percent': avg_change,
        'confidence': 'Moyenne' if abs(avg_change) < 5 else 'Faible' if abs(avg_change) > 10 else '√âlev√©e'
    }

# --- Sauvegarde dans PostgreSQL ---
def save_predictions_to_db(conn, company_id, symbol, prediction_data):
    """Sauvegarde les pr√©dictions dans la table predictions"""
    try:
        with conn.cursor() as cur:
            # Supprimer les anciennes pr√©dictions pour cette soci√©t√©
            cur.execute("DELETE FROM predictions WHERE company_id = %s", (company_id,))
            
            # Ins√©rer les nouvelles pr√©dictions
            for i, pred_date in enumerate(prediction_data['dates']):
                cur.execute("""
                    INSERT INTO predictions (
                        company_id, 
                        prediction_date, 
                        predicted_price, 
                        lower_bound, 
                        upper_bound,
                        confidence_level,
                        created_at
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP)
                """, (
                    company_id,
                    pred_date,
                    float(prediction_data['predictions'][i]),
                    float(prediction_data['lower_bound'][i]),
                    float(prediction_data['upper_bound'][i]),
                    prediction_data['confidence']
                ))
            
            conn.commit()
            logging.info(f"   ‚úÖ PostgreSQL: 20 pr√©dictions sauvegard√©es pour {symbol}")
            return True
    
    except Exception as e:
        logging.error(f"‚ùå Erreur sauvegarde pr√©dictions DB pour {symbol}: {e}")
        conn.rollback()
        return False

# --- Sauvegarde dans Google Sheets ---
def save_predictions_to_gsheet(gc, spreadsheet, symbol, prediction_data):
    """Sauvegarde les pr√©dictions dans une feuille d√©di√©e"""
    try:
        sheet_name = f"{symbol}_Predictions"
        
        # Cr√©er ou ouvrir la feuille
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            # Effacer le contenu existant
            worksheet.clear()
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=30, cols=6)
        
        # En-t√™tes
        headers = ['Date', 'Prix Pr√©dit', 'Borne Inf√©rieure', 'Borne Sup√©rieure', 'Confiance', 'Variation %']
        
        # Donn√©es
        rows = [headers]
        for i, pred_date in enumerate(prediction_data['dates']):
            variation = ((prediction_data['predictions'][i] - prediction_data['last_price']) / 
                        prediction_data['last_price'] * 100)
            
            rows.append([
                pred_date.strftime('%d/%m/%Y'),
                f"{prediction_data['predictions'][i]:.2f}",
                f"{prediction_data['lower_bound'][i]:.2f}",
                f"{prediction_data['upper_bound'][i]:.2f}",
                prediction_data['confidence'],
                f"{variation:.2f}%"
            ])
        
        # √âcrire toutes les donn√©es
        worksheet.update('A1', rows, value_input_option='USER_ENTERED')
        
        logging.info(f"   ‚úÖ Google Sheets: feuille {sheet_name} mise √† jour")
        return True
    
    except Exception as e:
        logging.error(f"‚ùå Erreur sauvegarde GSheet pr√©dictions pour {symbol}: {e}")
        return False

# --- Traitement par soci√©t√© ---
def process_company_prediction(conn, gc, spreadsheet, company_id, symbol):
    """G√©n√®re et sauvegarde les pr√©dictions pour une soci√©t√©"""
    logging.info(f"--- Pr√©diction: {symbol} ---")
    
    try:
        # R√©cup√©rer les 100 derniers jours de donn√©es
        query = """
            SELECT trade_date, price 
            FROM historical_data 
            WHERE company_id = %s 
            ORDER BY trade_date DESC 
            LIMIT 100
        """
        df = pd.read_sql(query, conn, params=(company_id,))
        
        if len(df) < 100:
            logging.warning(f"   ‚ö†Ô∏è  Pas assez de donn√©es ({len(df)} jours < 100)")
            return False
        
        # Inverser pour avoir du plus ancien au plus r√©cent
        df = df.iloc[::-1].reset_index(drop=True)
        
        # G√©n√©rer les pr√©dictions
        prediction_data = hybrid_prediction(df['price'], df['trade_date'])
        
        if prediction_data is None:
            return False
        
        # Afficher un r√©sum√©
        logging.info(f"   üìä Prix actuel: {prediction_data['last_price']:.2f} F CFA")
        logging.info(f"   üìä Prix pr√©dit J+20: {prediction_data['predictions'][-1]:.2f} F CFA")
        logging.info(f"   üìä Variation pr√©vue: {prediction_data['avg_change_percent']:.2f}%")
        logging.info(f"   üìä Confiance: {prediction_data['confidence']}")
        
        # Sauvegarder dans PostgreSQL
        save_predictions_to_db(conn, company_id, symbol, prediction_data)
        
        # Sauvegarder dans Google Sheets
        if gc and spreadsheet:
            save_predictions_to_gsheet(gc, spreadsheet, symbol, prediction_data)
        
        return True
    
    except Exception as e:
        logging.error(f"‚ùå Erreur pr√©diction {symbol}: {e}")
        return False

# --- Fonction principale ---
def run_prediction_analysis():
    logging.info("="*80)
    logging.info("üîÆ √âTAPE 3: PR√âDICTIONS (ALGORITHME HYBRIDE - 20 JOURS OUVR√âS)")
    logging.info("="*80)
    
    conn = connect_to_db()
    if not conn:
        return
    
    gc = authenticate_gsheets()
    spreadsheet = None
    
    if gc:
        try:
            spreadsheet = gc.open_by_key(SPREADSHEET_ID)
        except Exception as e:
            logging.error(f"‚ùå Erreur ouverture spreadsheet: {e}")
    
    try:
        # R√©cup√©rer toutes les soci√©t√©s
        with conn.cursor() as cur:
            cur.execute("SELECT id, symbol FROM companies ORDER BY symbol;")
            companies = cur.fetchall()
        
        logging.info(f"üìä {len(companies)} soci√©t√©(s) √† analyser")
        
        success_count = 0
        for company_id, symbol in companies:
            if process_company_prediction(conn, gc, spreadsheet, company_id, symbol):
                success_count += 1
        
        logging.info("\n" + "="*80)
        logging.info(f"‚úÖ Pr√©dictions termin√©es")
        logging.info(f"üìä Succ√®s: {success_count}/{len(companies)} soci√©t√©s")
        logging.info("="*80)
    
    except Exception as e:
        logging.error(f"‚ùå Erreur critique: {e}", exc_info=True)
    finally:
        if conn:
            conn.close()

if __name__ == "__main__":
    run_prediction_analysis()
