# ==============================================================================
# MODULE: DATA COLLECTOR (V3.1 - √âCRITURE SIMULTAN√âE DB + GOOGLE SHEETS)
# ==============================================================================

import re
import time
import unicodedata
import logging
import os
import json
from io import BytesIO
from datetime import datetime

import pdfplumber
import requests
from bs4 import BeautifulSoup
import psycopg2
import urllib3
import gspread
from google.oauth2 import service_account

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- Configuration & Secrets ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')

DB_NAME = os.environ.get('DB_NAME')
DB_USER = os.environ.get('DB_USER')
DB_PASSWORD = os.environ.get('DB_PASSWORD')
DB_HOST = os.environ.get('DB_HOST')
DB_PORT = os.environ.get('DB_PORT')
GSPREAD_SERVICE_ACCOUNT_JSON = os.environ.get('GSPREAD_SERVICE_ACCOUNT')
SPREADSHEET_ID = os.environ.get('SPREADSHEET_ID')

# --- Connexion PostgreSQL ---
def connect_to_db():
    try:
        conn = psycopg2.connect(
            dbname=DB_NAME, 
            user=DB_USER, 
            password=DB_PASSWORD, 
            host=DB_HOST, 
            port=DB_PORT
        )
        logging.info("‚úÖ Connexion √† la base de donn√©es PostgreSQL r√©ussie.")
        return conn
    except Exception as e:
        logging.error(f"‚ùå Impossible de se connecter √† la DB: {e}")
        return None

# --- Authentification Google Sheets ---
def authenticate_gsheets():
    try:
        creds_json = GSPREAD_SERVICE_ACCOUNT_JSON
        
        if not creds_json:
            logging.warning("‚ö†Ô∏è  GSPREAD_SERVICE_ACCOUNT non d√©fini, Google Sheets sera ignor√©")
            return None
        
        if not creds_json.strip().startswith('{'):
            logging.error("‚ùå GSPREAD_SERVICE_ACCOUNT ne contient pas un JSON valide")
            return None
        
        creds_dict = json.loads(creds_json)
        scopes = ['https://www.googleapis.com/auth/spreadsheets']
        creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=scopes)
        gc = gspread.authorize(creds)
        logging.info("‚úÖ Authentification Google Sheets r√©ussie.")
        return gc
    except json.JSONDecodeError as e:
        logging.error(f"‚ùå Erreur de parsing JSON : {e}")
        return None
    except Exception as e:
        logging.error(f"‚ùå Erreur d'authentification Google Sheets : {e}")
        return None

# --- R√©cup√©ration des IDs des soci√©t√©s ---
def get_company_ids(cur):
    try:
        cur.execute("SELECT symbol, id FROM companies;")
        return {row[0]: row[1] for row in cur.fetchall()}
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la r√©cup√©ration des IDs des soci√©t√©s : {e}")
        return {}

# --- Extraction de date depuis nom de fichier ---
def extract_date_from_filename_for_sorting(url):
    date_match = re.search(r'(\d{8})', url)
    if date_match:
        return date_match.group(1)
    return '19000101'

# --- R√©cup√©ration des liens BOC ---
def get_boc_links():
    url = "https://www.brvm.org/fr/bulletins-officiels-de-la-cote"
    logging.info(f"Recherche de bulletins sur : {url}")
    r = requests.get(url, verify=False, timeout=30)
    soup = BeautifulSoup(r.content, 'html.parser')
    links = set()
    
    for a in soup.find_all('a', href=True):
        href = a['href'].strip()
        if 'boc_' in href.lower() and href.endswith('.pdf'):
            full_url = href if href.startswith('http') else "https://www.brvm.org" + href
            links.add(full_url)
    
    if not links:
        logging.warning("Aucun lien de bulletin (BOC) trouv√© sur la page principale.")

    sorted_links = sorted(list(links), key=extract_date_from_filename_for_sorting, reverse=True)
    return sorted_links[:15]

# --- Nettoyage et conversion num√©rique ---
def clean_and_convert_numeric(value):
    if value is None or value == '': 
        return None
    cleaned_value = re.sub(r'\s+', '', str(value)).replace(',', '.')
    try: 
        return float(cleaned_value)
    except (ValueError, TypeError): 
        return None

# --- Extraction des donn√©es depuis PDF ---
def extract_data_from_pdf(pdf_url):
    logging.info(f"Analyse du PDF : {pdf_url}")
    data = []
    try:
        r = requests.get(pdf_url, verify=False, timeout=30)
        pdf_file = BytesIO(r.content)
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                tables = page.extract_tables() or []
                for table in tables:
                    for row in table:
                        row = [(cell.strip() if cell else "") for cell in row]
                        if len(row) < 8: 
                            continue
                        vol, val = row[-8], row[-7]
                        cours = row[-6] if len(row) >= 6 else ""
                        symbole = row[1] if len(row) > 1 and row[1] and len(row[1]) <= 5 else row[0]
                        if re.search(r'\d', str(vol)) or re.search(r'\d', str(val)):
                            data.append({
                                "Symbole": symbole, 
                                "Cours": cours, 
                                "Volume": vol, 
                                "Valeur": val
                            })
    except Exception as e:
        logging.error(f"Erreur lors de l'extraction des donn√©es du PDF {pdf_url}: {e}")
    return data

# --- √âcriture dans Google Sheets ---
def write_to_gsheet(gc, symbol, trade_date, price, volume):
    """√âcrit une ligne dans Google Sheets pour un symbole donn√©."""
    if not gc or not SPREADSHEET_ID:
        return False
    
    try:
        spreadsheet = gc.open_by_key(SPREADSHEET_ID)
        
        try:
            worksheet = spreadsheet.worksheet(symbol)
        except gspread.exceptions.WorksheetNotFound:
            # Cr√©er la feuille si elle n'existe pas
            worksheet = spreadsheet.add_worksheet(title=symbol, rows=1000, cols=10)
            worksheet.update([['Symbol', 'Date', 'Price', 'Volume']])
            logging.info(f"  üìÑ Feuille '{symbol}' cr√©√©e dans Google Sheets")
        
        # V√©rifier si la date existe d√©j√†
        existing_data = worksheet.get_all_values()
        date_str = trade_date.strftime('%d/%m/%Y')
        
        for row in existing_data[1:]:  # Ignorer l'en-t√™te
            if len(row) > 1 and row[1] == date_str:
                return True  # D√©j√† pr√©sent, pas besoin d'ajouter
        
        # Ajouter la nouvelle ligne
        worksheet.append_row([
            symbol,
            date_str,
            price if price else '',
            volume if volume else ''
        ], value_input_option='USER_ENTERED')
        
        return True
    except Exception as e:
        logging.error(f"  ‚ùå Erreur Google Sheets pour {symbol}: {e}")
        return False

# --- Fonction principale de collecte ---
def run_data_collection():
    logging.info("="*60)
    logging.info("√âTAPE 1 : D√âMARRAGE DE LA COLLECTE DE DONN√âES (V3.0)")
    logging.info("√âCRITURE SIMULTAN√âE : PostgreSQL + Google Sheets")
    logging.info("="*60)
    
    conn = connect_to_db()
    if not conn: 
        return
    
    gc = authenticate_gsheets()
    if not gc:
        logging.warning("‚ö†Ô∏è  Google Sheets non disponible, seule PostgreSQL sera utilis√©e")
    
    try:
        with conn.cursor() as cur:
            company_ids = get_company_ids(cur)
        
        boc_links = get_boc_links()
        logging.info(f"{len(boc_links)} BOCs r√©cents trouv√©s sur le site.")
        
        total_new_records_db = 0
        total_new_records_gsheet = 0
        
        for boc in boc_links:
            date_match = re.search(r'(\d{8})', boc)
            if not date_match: 
                continue
            date_yyyymmdd = date_match.group(1)

            try:
                trade_date = datetime.strptime(date_yyyymmdd, '%Y%m%d').date()
            except ValueError:
                continue

            # V√©rifier si les donn√©es existent d√©j√† en DB
            with conn.cursor() as cur:
                cur.execute("SELECT 1 FROM historical_data WHERE trade_date = %s LIMIT 1;", (trade_date,))
                if cur.fetchone():
                    logging.info(f"Les donn√©es pour la date {trade_date} existent d√©j√†. Ignor√©.")
                    continue
            
            logging.info(f"üìÖ Traitement des donn√©es pour la date {trade_date}...")
            rows = extract_data_from_pdf(boc)
            if not rows: 
                continue

            new_records_for_this_date_db = 0
            new_records_for_this_date_gsheet = 0
            
            with conn.cursor() as cur:
                for rec in rows:
                    symbol = rec.get('Symbole', '').strip()
                    if symbol in company_ids:
                        company_id = company_ids[symbol]
                        try:
                            price = clean_and_convert_numeric(rec.get('Cours'))
                            volume = int(clean_and_convert_numeric(rec.get('Volume')) or 0)
                            value = clean_and_convert_numeric(rec.get('Valeur'))
                            
                            # 1Ô∏è‚É£ √âCRITURE DANS POSTGRESQL
                            cur.execute("""
                                INSERT INTO historical_data (company_id, trade_date, price, volume, value)
                                VALUES (%s, %s, %s, %s, %s)
                                ON CONFLICT (company_id, trade_date) DO NOTHING;
                            """, (company_id, trade_date, price, volume, value))
                            
                            if cur.rowcount > 0:
                                new_records_for_this_date_db += 1
                                
                                # 2Ô∏è‚É£ √âCRITURE DANS GOOGLE SHEETS (simultan√©e)
                                if gc:
                                    if write_to_gsheet(gc, symbol, trade_date, price, volume):
                                        new_records_for_this_date_gsheet += 1
                                
                        except (ValueError, TypeError):
                            pass  # Ignorer les donn√©es invalides

            if new_records_for_this_date_db > 0 or new_records_for_this_date_gsheet > 0:
                logging.info(f"  ‚úÖ {trade_date}:")
                logging.info(f"     ‚Ä¢ PostgreSQL: {new_records_for_this_date_db} enregistrements")
                if gc:
                    logging.info(f"     ‚Ä¢ Google Sheets: {new_records_for_this_date_gsheet} enregistrements")
                total_new_records_db += new_records_for_this_date_db
                total_new_records_gsheet += new_records_for_this_date_gsheet
            else:
                logging.info(f"  -> Aucune nouvelle donn√©e pour le {trade_date}")

            conn.commit()

        logging.info("\n" + "="*60)
        logging.info(f"‚úÖ COLLECTE TERMIN√âE")
        logging.info(f"   üìä PostgreSQL: {total_new_records_db} nouveaux enregistrements")
        if gc:
            logging.info(f"   üìä Google Sheets: {total_new_records_gsheet} nouveaux enregistrements")
        logging.info("="*60)

    except Exception as e:
        logging.error(f"‚ùå Erreur critique dans la collecte de donn√©es : {e}", exc_info=True)
        if conn: 
            conn.rollback()
    finally:
        if conn: 
            conn.close()
    
    logging.info("Processus de collecte de donn√©es termin√©.")

if __name__ == "__main__":
    run_data_collection()
