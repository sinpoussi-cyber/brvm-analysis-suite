# ==============================================================================
# MODULE: DATA COLLECTOR (V2.6 - VERSION CORRIG√âE)
# ==============================================================================

import re
import time
import unicodedata
import logging
import os
from io import BytesIO
from datetime import datetime

import pdfplumber
import requests
from bs4 import BeautifulSoup
import psycopg2
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- Configuration & Secrets ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
DB_NAME = os.environ.get('DB_NAME')
DB_USER = os.environ.get('DB_USER')
DB_PASSWORD = os.environ.get('DB_PASSWORD')
DB_HOST = os.environ.get('DB_HOST')
DB_PORT = os.environ.get('DB_PORT')

def connect_to_db():
    """√âtablit une connexion √† la base de donn√©es PostgreSQL."""
    try:
        conn = psycopg2.connect(
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD,
            host=DB_HOST,
            port=DB_PORT
        )
        logging.info("‚úÖ Connexion √† la base de donn√©es PostgreSQL r√©ussie.")
        return conn
    except Exception as e:
        logging.error(f"‚ùå Impossible de se connecter √† la DB: {e}")
        return None

def get_company_ids(cur):
    """R√©cup√®re le mapping symbole -> ID depuis la table companies."""
    try:
        cur.execute("SELECT symbol, id FROM companies;")
        return {row[0]: row[1] for row in cur.fetchall()}
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la r√©cup√©ration des IDs des soci√©t√©s : {e}")
        return {}

def extract_date_from_filename_for_sorting(url):
    """Extrait une date num√©rique YYYYMMDD pour le tri, avec validation."""
    date_match = re.search(r'(\d{8})', url)
    if date_match:
        date_str = date_match.group(1)
        # Validation de la date
        try:
            datetime.strptime(date_str, '%Y%m%d')
            return date_str
        except ValueError:
            logging.warning(f"Date invalide extraite: {date_str} de {url}")
    
    # Si pas de date valide, utiliser une valeur de tri tr√®s ancienne
    logging.debug(f"Impossible d'extraire une date valide de: {url}")
    return '19000101'

def get_boc_links():
    """R√©cup√®re les liens vers les bulletins officiels de la cote (BOC)."""
    url = "https://www.brvm.org/fr/bulletins-officiels-de-la-cote"
    logging.info(f"üîç Recherche de bulletins sur : {url}")
    
    try:
        r = requests.get(url, verify=False, timeout=30)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, 'html.parser')
        links = set()
        
        for a in soup.find_all('a', href=True):
            href = a['href'].strip()
            # CORRECTION : Accepter tous les PDF qui contiennent 'boc_' (avec ou sans suffixe _2)
            if 'boc_' in href.lower() and href.endswith('.pdf'):
                full_url = href if href.startswith('http') else "https://www.brvm.org" + href
                links.add(full_url)
        
        if not links:
            logging.warning("‚ö†Ô∏è Aucun lien de bulletin (BOC) trouv√© sur la page principale.")
            return []
        
        # Tri par date d√©croissante (plus r√©cent en premier) pour l'affichage
        # MAIS on va inverser apr√®s pour traiter du plus ancien au plus r√©cent
        sorted_links = sorted(list(links), key=extract_date_from_filename_for_sorting, reverse=True)
        
        logging.info(f"‚úÖ {len(sorted_links)} bulletins identifi√©s sur le site (incluant les versions _2)")
        
        # Retourner TOUS les bulletins (pas de limite)
        return sorted_links
    
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la r√©cup√©ration des liens BOC : {e}")
        return []

def clean_and_convert_numeric(value):
    """Nettoie et convertit une valeur en nombre flottant."""
    if value is None or value == '':
        return None
    
    cleaned_value = re.sub(r'\s+', '', str(value)).replace(',', '.')
    
    try:
        return float(cleaned_value)
    except (ValueError, TypeError):
        return None

def extract_data_from_pdf(pdf_url):
    """Extrait les donn√©es de march√© d'un PDF BOC."""
    logging.info(f"üìÑ Analyse du PDF : {os.path.basename(pdf_url)}")
    data = []
    
    try:
        r = requests.get(pdf_url, verify=False, timeout=30)
        r.raise_for_status()
        pdf_file = BytesIO(r.content)
        
        with pdfplumber.open(pdf_file) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                tables = page.extract_tables() or []
                
                for table in tables:
                    for row in table:
                        # Nettoyer les cellules
                        row = [(cell.strip() if cell else "") for cell in row]
                        
                        # Ignorer les lignes trop courtes
                        if len(row) < 8:
                            continue
                        
                        # Les colonnes Volume et Valeur sont typiquement √† -8 et -7
                        vol = row[-8]
                        val = row[-7]
                        cours = row[-6] if len(row) >= 6 else ""
                        
                        # Le symbole est g√©n√©ralement dans les premi√®res colonnes
                        symbole = row[1] if len(row) > 1 and row[1] and len(row[1]) <= 5 else row[0]
                        
                        # Garder seulement les lignes avec des donn√©es num√©riques
                        if re.search(r'\d', str(vol)) or re.search(r'\d', str(val)):
                            data.append({
                                "Symbole": symbole,
                                "Cours": cours,
                                "Volume": vol,
                                "Valeur": val
                            })
        
        logging.info(f"   ‚úì {len(data)} lignes de donn√©es extraites")
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de l'extraction des donn√©es du PDF : {e}")
    
    return data

def run_data_collection():
    """Fonction principale pour la collecte de donn√©es."""
    logging.info("=" * 60)
    logging.info("√âTAPE 1 : D√âMARRAGE DE LA COLLECTE DE DONN√âES (V2.6)")
    logging.info("=" * 60)
    
    conn = connect_to_db()
    if not conn:
        return
    
    try:
        # R√©cup√©rer le mapping des IDs de soci√©t√©s
        with conn.cursor() as cur:
            company_ids = get_company_ids(cur)
        
        if not company_ids:
            logging.error("‚ùå Aucune soci√©t√© trouv√©e dans la base de donn√©es.")
            return
        
        logging.info(f"üìä {len(company_ids)} soci√©t√©s identifi√©es dans la base")
        
        # R√©cup√©rer les liens vers les BOCs
        boc_links = get_boc_links()
        
        if not boc_links:
            logging.warning("‚ö†Ô∏è Aucun bulletin trouv√©. Fin de la collecte.")
            return
        
        logging.info(f"‚úÖ {len(boc_links)} BOCs identifi√©s et tri√©s (du plus ancien au plus r√©cent)")
        
        total_new_records = 0
        dates_processed = []
        
        # Traiter chaque bulletin
        for idx, boc in enumerate(boc_links, 1):
            # Extraire la date du nom du fichier
            date_match = re.search(r'(\d{8})', boc)
            if not date_match:
                logging.warning(f"   ‚ö†Ô∏è Impossible d'extraire la date de {boc}. Ignor√©.")
                continue
            
            date_yyyymmdd = date_match.group(1)
            
            try:
                trade_date = datetime.strptime(date_yyyymmdd, '%Y%m%d').date()
            except ValueError:
                logging.warning(f"   ‚ö†Ô∏è Format de date invalide : {date_yyyymmdd}. Ignor√©.")
                continue
            
            # V√©rifier si on a d√©j√† des donn√©es pour cette date
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM historical_data WHERE trade_date = %s;", (trade_date,))
                count = cur.fetchone()[0]
                
                if count > 0:
                    logging.info(f"   ‚è≠Ô∏è  [{idx}/{len(boc_links)}] {trade_date} : Donn√©es d√©j√† pr√©sentes ({count} enregistrements). Ignor√©.")
                    continue
            
            logging.info(f"   üîÑ [{idx}/{len(boc_links)}] Traitement des donn√©es pour le {trade_date}...")
            
            # Extraire les donn√©es du PDF
            rows = extract_data_from_pdf(boc)
            
            if not rows:
                logging.warning(f"      ‚ö†Ô∏è Aucune donn√©e extraite du bulletin.")
                continue
            
            # Ins√©rer les donn√©es dans la base
            new_records_for_this_date = 0
            
            with conn.cursor() as cur:
                for rec in rows:
                    symbol = rec.get('Symbole', '').strip().upper()
                    
                    if symbol not in company_ids:
                        continue  # Ignorer les symboles non reconnus
                    
                    company_id = company_ids[symbol]
                    
                    try:
                        price = clean_and_convert_numeric(rec.get('Cours'))
                        volume = int(clean_and_convert_numeric(rec.get('Volume')) or 0)
                        value = clean_and_convert_numeric(rec.get('Valeur'))
                        
                        cur.execute("""
                            INSERT INTO historical_data (company_id, trade_date, price, volume, value)
                            VALUES (%s, %s, %s, %s, %s)
                            ON CONFLICT (company_id, trade_date) DO NOTHING;
                        """, (company_id, trade_date, price, volume, value))
                        
                        if cur.rowcount > 0:
                            new_records_for_this_date += 1
                    
                    except (ValueError, TypeError) as e:
                        # Ignorer silencieusement les erreurs de conversion
                        logging.debug(f"      Erreur de conversion pour {symbol}: {e}")
                        continue
            
            # Commit apr√®s chaque date
            conn.commit()
            
            if new_records_for_this_date > 0:
                logging.info(f"      ‚úÖ {new_records_for_this_date} nouveaux enregistrements ajout√©s")
                total_new_records += new_records_for_this_date
                dates_processed.append(str(trade_date))
            else:
                logging.info(f"      ‚ÑπÔ∏è  Aucune nouvelle donn√©e (doublons ignor√©s)")
            
            # Petit d√©lai entre les t√©l√©chargements
            time.sleep(1)
        
        # R√©sum√© final
        logging.info("\n" + "=" * 60)
        logging.info("üìä R√âSUM√â DE LA COLLECTE")
        logging.info("=" * 60)
        logging.info(f"   ‚Ä¢ Bulletins trait√©s : {len(boc_links)}")
        logging.info(f"   ‚Ä¢ Dates ajout√©es : {len(dates_processed)}")
        logging.info(f"   ‚Ä¢ Total nouveaux enregistrements : {total_new_records}")
        
        if dates_processed:
            logging.info(f"   ‚Ä¢ Dates : {', '.join(dates_processed[-5:])}")  # Afficher les 5 derni√®res
        
        logging.info("=" * 60)
        logging.info("‚úÖ Processus de collecte de donn√©es termin√© avec succ√®s")
    
    except Exception as e:
        logging.error(f"‚ùå Erreur critique dans la collecte de donn√©es : {e}", exc_info=True)
        if conn:
            conn.rollback()
    
    finally:
        if conn:
            conn.close()

if __name__ == "__main__":
    run_data_collection()
