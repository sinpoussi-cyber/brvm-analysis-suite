# ==============================================================================
# MODULE: DATA COLLECTOR V4.0 - COLLECTE COMPL√àTE SANS DOUBLONS
# ==============================================================================

import re
import time
import logging
import os
import json
from io import BytesIO
from datetime import datetime, timedelta

import pdfplumber
import requests
from bs4 import BeautifulSoup
import psycopg2
import urllib3
import gspread
from google.oauth2 import service_account

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')

# --- Secrets ---
DB_NAME = os.environ.get('DB_NAME')
DB_USER = os.environ.get('DB_USER')
DB_PASSWORD = os.environ.get('DB_PASSWORD')
DB_HOST = os.environ.get('DB_HOST')
DB_PORT = os.environ.get('DB_PORT')
GSPREAD_SERVICE_ACCOUNT_JSON = os.environ.get('GSPREAD_SERVICE_ACCOUNT')
SPREADSHEET_ID = os.environ.get('SPREADSHEET_ID')

# --- Connexion DB ---
def connect_to_db():
    try:
        conn = psycopg2.connect(
            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, 
            host=DB_HOST, port=DB_PORT
        )
        logging.info("‚úÖ Connexion PostgreSQL r√©ussie.")
        return conn
    except Exception as e:
        logging.error(f"‚ùå Erreur connexion DB: {e}")
        return None

# --- Authentification Google Sheets ---
def authenticate_gsheets():
    try:
        if not GSPREAD_SERVICE_ACCOUNT_JSON:
            logging.warning("‚ö†Ô∏è  GSPREAD_SERVICE_ACCOUNT non d√©fini")
            return None
        
        creds_dict = json.loads(GSPREAD_SERVICE_ACCOUNT_JSON)
        scopes = ['https://www.googleapis.com/auth/spreadsheets']
        creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=scopes)
        gc = gspread.authorize(creds)
        logging.info("‚úÖ Authentification Google Sheets r√©ussie.")
        return gc
    except Exception as e:
        logging.error(f"‚ùå Erreur authentification Google Sheets: {e}")
        return None

# --- R√©cup√©ration des IDs soci√©t√©s ---
def get_company_ids(cur):
    try:
        cur.execute("SELECT symbol, id FROM companies;")
        return {row[0]: row[1] for row in cur.fetchall()}
    except Exception as e:
        logging.error(f"‚ùå Erreur r√©cup√©ration IDs soci√©t√©s: {e}")
        return {}

# --- Extraction date depuis URL ---
def extract_date_from_url(url):
    """Extrait la date au format YYYYMMDD depuis l'URL"""
    date_match = re.search(r'boc_(\d{8})', url)
    if date_match:
        return date_match.group(1)
    return None

# --- R√©cup√©ration TOUS les BOCs disponibles ---
def get_all_boc_links():
    """R√©cup√®re tous les BOCs disponibles sur le site"""
    url = "https://www.brvm.org/fr/bulletins-officiels-de-la-cote"
    logging.info(f"üîç Recherche de TOUS les BOCs sur : {url}")
    
    try:
        r = requests.get(url, verify=False, timeout=30)
        soup = BeautifulSoup(r.content, 'html.parser')
        links = set()
        
        for a in soup.find_all('a', href=True):
            href = a['href'].strip()
            if 'boc_' in href.lower() and href.endswith('.pdf'):
                full_url = href if href.startswith('http') else "https://www.brvm.org" + href
                links.add(full_url)
        
        if not links:
            logging.warning("‚ö†Ô∏è  Aucun BOC trouv√© sur la page")
            return []
        
        # Trier par date (du plus ancien au plus r√©cent)
        sorted_links = sorted(list(links), key=lambda x: extract_date_from_url(x) or '19000101')
        
        logging.info(f"‚úÖ {len(sorted_links)} BOC(s) trouv√©(s)")
        for link in sorted_links:
            date_str = extract_date_from_url(link)
            logging.info(f"   ‚Ä¢ BOC du {date_str}")
        
        return sorted_links
    
    except Exception as e:
        logging.error(f"‚ùå Erreur r√©cup√©ration BOCs: {e}")
        return []

# --- V√©rification existence date dans DB ---
def date_exists_in_db(conn, trade_date):
    """V√©rifie si des donn√©es existent d√©j√† pour cette date"""
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT 1 FROM historical_data WHERE trade_date = %s LIMIT 1;", (trade_date,))
            return cur.fetchone() is not None
    except Exception as e:
        logging.error(f"‚ùå Erreur v√©rification date DB: {e}")
        return False

# --- V√©rification existence date dans Google Sheets ---
def date_exists_in_gsheet(worksheet, trade_date):
    """V√©rifie si des donn√©es existent d√©j√† pour cette date dans la feuille"""
    try:
        date_str = trade_date.strftime('%d/%m/%Y')
        all_values = worksheet.col_values(2)  # Colonne B = Date
        return date_str in all_values
    except Exception as e:
        logging.error(f"‚ùå Erreur v√©rification date GSheet: {e}")
        return False

# --- Nettoyage valeurs num√©riques ---
def clean_and_convert_numeric(value):
    if value is None or value == '': 
        return None
    cleaned_value = re.sub(r'\s+', '', str(value)).replace(',', '.')
    try: 
        return float(cleaned_value)
    except (ValueError, TypeError): 
        return None

# --- Extraction donn√©es depuis PDF ---
def extract_data_from_pdf(pdf_url):
    """Extrait les donn√©es du BOC PDF"""
    logging.info(f"üìÑ Analyse du PDF: {os.path.basename(pdf_url)}")
    data = []
    
    try:
        r = requests.get(pdf_url, verify=False, timeout=30)
        pdf_file = BytesIO(r.content)
        
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                tables = page.extract_tables() or []
                for table in tables:
                    for row in table:
                        row = [(cell.strip() if cell else "") for cell in row]
                        if len(row) < 8: 
                            continue
                        
                        vol = row[-8]
                        val = row[-7]
                        cours = row[-6] if len(row) >= 6 else ""
                        symbole = row[1] if len(row) > 1 and row[1] and len(row[1]) <= 5 else row[0]
                        
                        if re.search(r'\d', str(vol)) or re.search(r'\d', str(val)):
                            data.append({
                                "Symbole": symbole, 
                                "Cours": cours, 
                                "Volume": vol, 
                                "Valeur": val
                            })
        
        logging.info(f"   ‚úì {len(data)} ligne(s) extraite(s)")
        return data
    
    except Exception as e:
        logging.error(f"‚ùå Erreur extraction PDF: {e}")
        return []

# --- Insertion dans DB ---
def insert_into_db(conn, company_ids, symbol, trade_date, price, volume, value):
    """Insert les donn√©es dans PostgreSQL"""
    if symbol not in company_ids:
        return False
    
    company_id = company_ids[symbol]
    
    try:
        with conn.cursor() as cur:
            cur.execute("""
                INSERT INTO historical_data (company_id, trade_date, price, volume, value)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (company_id, trade_date) DO NOTHING;
            """, (company_id, trade_date, price, volume, value))
            
            conn.commit()
            return cur.rowcount > 0
    
    except Exception as e:
        logging.error(f"‚ùå Erreur insertion DB pour {symbol}: {e}")
        conn.rollback()
        return False

# --- Insertion dans Google Sheets ---
def insert_into_gsheet(gc, spreadsheet, symbol, trade_date, price, volume, value):
    """Insert les donn√©es dans Google Sheets √† la bonne position chronologique"""
    try:
        worksheet = spreadsheet.worksheet(symbol)
        
        # Pr√©parer les donn√©es
        date_str = trade_date.strftime('%d/%m/%Y')
        new_row = [symbol, date_str, price if price else '', volume if volume else '', value if value else '']
        
        # R√©cup√©rer toutes les dates existantes
        all_values = worksheet.get_all_values()
        
        if len(all_values) <= 1:  # Seulement l'en-t√™te ou vide
            worksheet.append_row(new_row, value_input_option='USER_ENTERED')
            return True
        
        # Trouver la position d'insertion (ordre chronologique croissant)
        insert_position = None
        
        for idx, row in enumerate(all_values[1:], start=2):  # Commencer √† la ligne 2
            if len(row) < 2:
                continue
            
            try:
                existing_date = datetime.strptime(row[1], '%d/%m/%Y').date()
                
                # Si la date existe d√©j√†, ne rien faire
                if existing_date == trade_date:
                    return False
                
                # Si on trouve une date post√©rieure, ins√©rer avant
                if existing_date > trade_date:
                    insert_position = idx
                    break
            
            except:
                continue
        
        # Ins√©rer √† la position trouv√©e ou √† la fin
        if insert_position:
            worksheet.insert_row(new_row, insert_position, value_input_option='USER_ENTERED')
        else:
            worksheet.append_row(new_row, value_input_option='USER_ENTERED')
        
        return True
    
    except gspread.exceptions.WorksheetNotFound:
        logging.warning(f"‚ö†Ô∏è  Feuille '{symbol}' non trouv√©e")
        return False
    
    except Exception as e:
        logging.error(f"‚ùå Erreur insertion GSheet pour {symbol}: {e}")
        return False

# --- Nettoyage des feuilles "_Technical" ---
def cleanup_technical_sheets(gc, spreadsheet):
    """Supprime toutes les feuilles se terminant par '_Technical'"""
    try:
        worksheets = spreadsheet.worksheets()
        deleted_count = 0
        
        for ws in worksheets:
            if ws.title.endswith('_Technical'):
                logging.info(f"üóëÔ∏è  Suppression de la feuille: {ws.title}")
                spreadsheet.del_worksheet(ws)
                deleted_count += 1
                time.sleep(0.5)  # Pause pour respecter les limites API
        
        if deleted_count > 0:
            logging.info(f"‚úÖ {deleted_count} feuille(s) '_Technical' supprim√©e(s)")
        else:
            logging.info("‚ÑπÔ∏è  Aucune feuille '_Technical' √† supprimer")
    
    except Exception as e:
        logging.error(f"‚ùå Erreur nettoyage feuilles: {e}")

# --- Fonction principale ---
def run_data_collection():
    logging.info("="*60)
    logging.info("üìä √âTAPE 1: COLLECTE COMPL√àTE DES DONN√âES (V4.0)")
    logging.info("="*60)
    
    # Connexions
    conn = connect_to_db()
    if not conn:
        return
    
    gc = authenticate_gsheets()
    if not gc:
        logging.error("‚ùå Google Sheets requis, arr√™t du processus")
        conn.close()
        return
    
    try:
        spreadsheet = gc.open_by_key(SPREADSHEET_ID)
        
        # Nettoyage des feuilles "_Technical"
        cleanup_technical_sheets(gc, spreadsheet)
        
        # R√©cup√©ration des IDs soci√©t√©s
        with conn.cursor() as cur:
            company_ids = get_company_ids(cur)
        
        # R√©cup√©ration de TOUS les BOCs
        boc_links = get_all_boc_links()
        
        if not boc_links:
            logging.error("‚ùå Aucun BOC trouv√©")
            return
        
        total_db_inserts = 0
        total_gsheet_inserts = 0
        total_skipped = 0
        
        # Traiter chaque BOC (du plus ancien au plus r√©cent)
        for boc_url in boc_links:
            date_str = extract_date_from_url(boc_url)
            
            if not date_str:
                continue
            
            try:
                trade_date = datetime.strptime(date_str, '%Y%m%d').date()
            except ValueError:
                continue
            
            logging.info(f"\nüìÖ Traitement du BOC du {trade_date.strftime('%d/%m/%Y')}")
            
            # V√©rifier si cette date existe d√©j√† dans DB
            if date_exists_in_db(conn, trade_date):
                logging.info(f"   ‚è≠Ô∏è  Date d√©j√† pr√©sente dans DB, passage au suivant")
                total_skipped += 1
                continue
            
            # Extraire les donn√©es du PDF
            rows = extract_data_from_pdf(boc_url)
            
            if not rows:
                logging.warning(f"   ‚ö†Ô∏è  Aucune donn√©e extraite pour {trade_date}")
                continue
            
            db_inserts = 0
            gsheet_inserts = 0
            
            # Traiter chaque ligne
            for rec in rows:
                symbol = rec.get('Symbole', '').strip()
                
                if symbol not in company_ids:
                    continue
                
                try:
                    price = clean_and_convert_numeric(rec.get('Cours'))
                    volume = int(clean_and_convert_numeric(rec.get('Volume')) or 0)
                    value = clean_and_convert_numeric(rec.get('Valeur'))
                    
                    # Insertion DB
                    if insert_into_db(conn, company_ids, symbol, trade_date, price, volume, value):
                        db_inserts += 1
                    
                    # Insertion Google Sheets
                    if insert_into_gsheet(gc, spreadsheet, symbol, trade_date, price, volume, value):
                        gsheet_inserts += 1
                    
                    time.sleep(0.1)  # Petite pause entre insertions
                
                except Exception as e:
                    logging.error(f"   ‚ùå Erreur traitement {symbol}: {e}")
                    continue
            
            total_db_inserts += db_inserts
            total_gsheet_inserts += gsheet_inserts
            
            logging.info(f"   ‚úÖ DB: {db_inserts} | GSheet: {gsheet_inserts}")
            time.sleep(1)  # Pause entre BOCs
        
        # R√©sum√© final
        logging.info("\n" + "="*60)
        logging.info("‚úÖ COLLECTE TERMIN√âE")
        logging.info(f"üìä BOCs trait√©s: {len(boc_links)}")
        logging.info(f"üìä BOCs ignor√©s (doublons): {total_skipped}")
        logging.info(f"üíæ PostgreSQL: {total_db_inserts} nouveaux enregistrements")
        logging.info(f"üìã Google Sheets: {total_gsheet_inserts} nouveaux enregistrements")
        logging.info("="*60)
    
    except Exception as e:
        logging.error(f"‚ùå Erreur critique: {e}", exc_info=True)
        if conn:
            conn.rollback()
    
    finally:
        if conn:
            conn.close()

if __name__ == "__main__":
    run_data_collection()
